{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e47a080-dee6-48fb-9e51-0749a40ec6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompts import Prompt, test_all\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146858-6aec-4030-ae65-0901bc581304",
   "metadata": {},
   "source": [
    "# Command-line test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad2724c-a48c-4851-a7fb-fb95bc68d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Mistral mistral-large-latest\n",
      "System Prompt: You are a helpful, harmless and honest assistant who has to answer questions in a funny style.\n",
      "Temperature: 0.0\n",
      "Max Tokens: 256\n",
      "Chat History: [('Who was Albert Einstein?', '')]\n",
      "Prompt: [ChatMessage(role='system', content='You are a helpful, harmless and honest assistant who has to answer questions in a funny style.', name=None, tool_calls=None),\n",
      " ChatMessage(role='user', content='Who was Albert Einstein?', name=None, tool_calls=None)]\n",
      "\n",
      "Oh, buckle up, because we're about to take a wild ride through the annals of history! Albert Einstein, my friend, was no ordinary Joe. He was the Picasso of physics, the Mozart of mathematics, the Shakespeare of science! Born in 1879, this German-born theoretical physicist was so brilliant, he made his parents look like they'd been skipping leg day at the brain gym.\n",
      "\n",
      "Now, you might be thinking, \"What's so special about this Einstein fellow?\" Well, let me tell you, he didn't just invent the polka-dotted bow tie, no siree! He came up with this little thing called the theory of relativity. You know, the whole E=mc^2 shebang? That's right, he figured out that energy and mass are interchangeable, and in the process, he changed the way we understand the universe.\n",
      "\n",
      "And if that wasn't enough, he also won a Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect. I mean, talk about overachieving, right?\n",
      "\n",
      "But despite his towering intellect,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt.set_system_prompt('You are a helpful, honest and honest assistant.')\n",
    "# Prompt.set_system_prompt('You are a funny assistant speaking in pirate English')\n",
    "Prompt.set_system_prompt('You are a helpful, harmless and honest assistant who has to answer questions in a funny style.')\n",
    "\n",
    "\n",
    "# You can use the LSP or the typing to check the available models\n",
    "#Prompt.set_model(\"GPT\", \"gpt-3.5-turbo\")\n",
    "Prompt.set_model(\"Mistral\", \"mistral-large\")\n",
    "#Prompt.set_model(\"Llama\", \"llama-2-70b-chat\")\n",
    "\n",
    "Prompt.set_max_tokens(256)\n",
    "Prompt.set_verbose(True)\n",
    "Prompt.set_temperature(0.00)\n",
    "\n",
    "streaming = True\n",
    "\n",
    "prompt = 'Who was Albert Einstein?'\n",
    "\n",
    "if streaming == True:\n",
    "    stream = Prompt.exec_streaming(prompt)\n",
    "    for chunk in stream:\n",
    "        print(chunk, end=\"\")\n",
    "    print(\"\")\n",
    "else:\n",
    "    response = Prompt.exec(prompt)\n",
    "    print(response)\n",
    "    print(\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712a037-a593-4b3b-9ba5-ceb58dc0351f",
   "metadata": {},
   "source": [
    "# Gradio program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a255bdf-4f95-4ea8-987e-e72f0cf5433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "Running on public URL: https://60bfeb3a4b2aa6ce56.gradio.componentsoft.ai\n",
      "\n",
      "This share link expires in 168 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://60bfeb3a4b2aa6ce56.gradio.componentsoft.ai\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "Prompt.set_verbose(True)\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"GPT\": [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
    "    \"Mistral\": [\"mistral-tiny\", \"mistral-small\", \"mistral-medium\", \"mistral-large\"],\n",
    "    \"Llama\": [\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\", \"codellama-7b-instruct\", \"codellama-70b-instruct\"],\n",
    "}\n",
    "\n",
    "def exec_prompt(chat_history, prompt, system_prompt, model_family = \"Mistral\", model=\"mistral-large\", temperature=0.7, max_tokens=512):\n",
    "    if prompt == \"\": prompt = \"I have no question\"\n",
    "    if model == \"mistral-large\": model = \"mistral-large-latest\"\n",
    "    if model_family == \"Mistral\" and temperature > 1: temperature = 1\n",
    "    if model_family == \"Llama\" and temperature < 0.01: temperature = 0.01\n",
    "    Prompt.set_model(model_family, model)\n",
    "    Prompt.set_system_prompt(system_prompt)\n",
    "    Prompt.set_temperature(temperature)\n",
    "    Prompt.set_max_tokens(max_tokens)    \n",
    "    \n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append([prompt, \"\"])\n",
    "    response = Prompt.exec(chat_history)\n",
    "    chat_history[-1][1] = response\n",
    "    return chat_history, \"\"\n",
    "\n",
    "def exec_prompt_streaming(chat_history, prompt, system_prompt, model_family = \"Mistral\", model=\"mistral-large\", temperature=0.7, max_tokens=512):\n",
    "    if prompt == \"\": prompt = \"I have no question\"\n",
    "    if model == \"mistral-large\": model = \"mistral-large-latest\"\n",
    "    if model_family == \"Mistral\" and temperature > 1: temperature = 1\n",
    "    if model_family == \"Llama\" and temperature < 0.01: temperature = 0.01\n",
    "    Prompt.set_system_prompt(system_prompt)\n",
    "    Prompt.set_temperature(temperature)\n",
    "    Prompt.set_max_tokens(max_tokens)\n",
    "    Prompt.set_model(model_family, model)\n",
    "    \n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append([prompt, \"\"])\n",
    "    stream = Prompt.exec_streaming(chat_history)\n",
    "    for new_token in stream:\n",
    "        if new_token is not None:\n",
    "            chat_history[-1][1] += str(new_token)\n",
    "        yield chat_history, \"\"\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    gr.Markdown(\"# Ericsson LLM Demo\")\n",
    "    system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"Mistral\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"Mistral\"]), label=\"Model\", value=\"mistral-large\")       \n",
    "        temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\n",
    "    with gr.Row():\n",
    "        chatbot=gr.Chatbot(label=\"ComponentSoft_GPT\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Your prompt\", value=\"Who was Albert Einstein?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer without streaming\")\n",
    "        submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot,prompt, system_prompt,modelfamily,model,temperature,max_tokens], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot,prompt,system_prompt, modelfamily,model,temperature,max_tokens], outputs=[chatbot, prompt])\n",
    "\n",
    "    callback.setup([system_prompt, modelfamily, model, temperature, max_tokens, chatbot], \"flagged_data_points\")\n",
    "    flag_btn.click(lambda *args: callback.flag(args), [system_prompt, modelfamily, model, temperature, max_tokens, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"Who was Albert Einstein?\", \"When did he live?\", \"What were a few of his most important achievements?\", \"Who were some other important personality from his profession and his age?\",\n",
    "        \"Write a Python function which calculates the value of PI in N steps with maximum precision using float64 numbers.\", \"Write the same function in Typescript.\", \n",
    "         \"The same in Java?\", \"And what about C#?\", \"In Fortran?\", \"In Cobol?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, favicon_path=\"/home/rconsole/GIT/AI-434/source/labfiles/data/favicon.ico\", state_session_capacity=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52a32432-451a-4e06-87a6-52e5708d5202",
   "metadata": {},
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4a848-2f93-4150-9420-8dd17840ccc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
