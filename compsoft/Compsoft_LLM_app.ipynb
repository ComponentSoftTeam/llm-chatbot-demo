{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47a080-dee6-48fb-9e51-0749a40ec6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_prompts import Prompt, test_all\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b3f3b-32a8-4783-ad15-5ffd588e5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Gradio_user = os.environ[\"GRADIO_USER\"]\n",
    "Gradio_password = os.environ[\"GRADIO_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712a037-a593-4b3b-9ba5-ceb58dc0351f",
   "metadata": {},
   "source": [
    "# Gradio program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a255bdf-4f95-4ea8-987e-e72f0cf5433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "Prompt.set_verbose(False)\n",
    "\n",
    "modelfamilies_model_dict = {\n",
    "    \"GPT\": [\"gpt-3.5-turbo\", \"gpt-4o\", \"gpt-4-turbo\"],\n",
    "    \"Mistral\": [\"mistral-small\", \"mistral-large\", \"open-mistral-7b\", \"open-mixtral-8x7b\", \"open-mixtral-8x22b\"],\n",
    "    #\"Llama\": [\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"llama-2-70b-chat\", \"codellama-7b-instruct\", \"codellama-70b-instruct\"],\n",
    "    \"Llama\": [\"llama-v3-8b-instruct\", \"llama-v3-70b-instruct\", \"llama-v2-7b-chat\", \"llama-v2-70b-chat\"],\n",
    "}\n",
    "\n",
    "def exec_prompt(chat_history, prompt, system_prompt, model_family = \"Mistral\", model=\"mistral-large\", temperature=0.7, max_tokens=512):\n",
    "    if prompt == \"\": prompt = \"I have no question\"\n",
    "    if model == \"mistral-large\": model = \"mistral-large-latest\"\n",
    "    if model == \"mistral-small\": model = \"mistral-small-latest\"\n",
    "    if model_family == \"Mistral\" and temperature > 1: temperature = 1\n",
    "    if model_family == \"Llama\" and temperature < 0.01: temperature = 0.01\n",
    "    Prompt.set_model(model_family, model)\n",
    "    Prompt.set_system_prompt(system_prompt)\n",
    "    Prompt.set_temperature(temperature)\n",
    "    Prompt.set_max_tokens(max_tokens)    \n",
    "  \n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append([prompt, \"\"])\n",
    "    response = Prompt.exec(chat_history)\n",
    "    chat_history[-1][1] = response\n",
    "    return chat_history, \"\"\n",
    "\n",
    "def exec_prompt_streaming(chat_history, prompt, system_prompt, model_family = \"Mistral\", model=\"mistral-large\", temperature=0.7, max_tokens=512):\n",
    "    if prompt == \"\": prompt = \"I have no question\"\n",
    "    if model == \"mistral-large\": model = \"mistral-large-latest\"\n",
    "    if model == \"mistral-small\": model = \"mistral-small-latest\"\n",
    "    if model_family == \"Mistral\" and temperature > 1: temperature = 1\n",
    "    if model_family == \"Llama\" and temperature < 0.01: temperature = 0.01\n",
    "    Prompt.set_system_prompt(system_prompt)\n",
    "    Prompt.set_temperature(temperature)\n",
    "    Prompt.set_max_tokens(max_tokens)\n",
    "    Prompt.set_model(model_family, model)\n",
    "    \n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append([prompt, \"\"])\n",
    "    stream = Prompt.exec_streaming(chat_history)\n",
    "    for new_token in stream:\n",
    "        if new_token is not None:\n",
    "            chat_history[-1][1] += str(new_token)\n",
    "        yield chat_history, \"\"\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    gr.Markdown(\"# Component Soft LLM Demo\")\n",
    "    system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"Mistral\")\n",
    "        model = gr.Dropdown(list(modelfamilies_model_dict[\"Mistral\"]), label=\"Model\", value=\"mistral-large\")       \n",
    "        temperature = gr.Slider(label=\"Temperature:\", minimum=0, maximum=2, value=1,\n",
    "            info=\"LLM generation temperature\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", minimum=100, maximum=2000, value=500, \n",
    "            info=\"Maximum number of generated tokens\")\n",
    "    with gr.Row():\n",
    "        chatbot=gr.Chatbot(label=\"ComponentSoft_GPT\", height=400, show_copy_button=True)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Your prompt\", value=\"Who was Albert Einstein?\")\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer without streaming\")\n",
    "        submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        flag_btn = gr.Button(\"Flag\")\n",
    "    \n",
    "    \n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model, value=model[0], interactive=True)\n",
    "\n",
    "    submit_btn_streaming.click(exec_prompt_streaming, inputs=[chatbot,prompt, system_prompt,modelfamily,model,temperature,max_tokens], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_prompt, inputs=[chatbot,prompt,system_prompt, modelfamily,model,temperature,max_tokens], outputs=[chatbot, prompt])\n",
    "\n",
    "    callback.setup([system_prompt, modelfamily, model, temperature, max_tokens, chatbot], \"flagged_data_points\")\n",
    "    flag_btn.click(lambda *args: callback.flag(args), [system_prompt, modelfamily, model, temperature, max_tokens, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"Who was Albert Einstein?\", \"When did he live?\", \"What were a few of his most important achievements?\", \"Who were some other important personality from his profession and his age?\",\n",
    "        \"Write a Python function which calculates the value of PI in N steps with maximum precision using float64 numbers.\", \"Write the same function in Typescript.\", \n",
    "         \"The same in Java?\", \"And what about C#?\", \"In Fortran?\", \"In Cobol?\"],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch()\n",
    "demo.launch(share=True)\n",
    "#demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(Gradio_user, Gradio_password), max_threads=20, show_error=True, favicon_path=\"data/favicon.ico\", state_session_capacity=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "985e1a2e-40f9-4955-a2cd-8183f133c5d5",
   "metadata": {},
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a9634-f530-4ce5-b3b8-0e10ee0e0a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
