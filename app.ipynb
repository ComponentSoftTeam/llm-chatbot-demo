{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47a080-dee6-48fb-9e51-0749a40ec6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from typing import Iterable\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "from langchain_fireworks.chat_models import ChatFireworks\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# The user for the Gradio interface\n",
    "GRADIO_USER = os.environ[\"GRADIO_USER\"]\n",
    "GRADIO_PASSWORD = os.environ[\"GRADIO_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123bdf5",
   "metadata": {},
   "source": [
    "# Setup the models\n",
    "\n",
    "Set up the model families, models, and their constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7be365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFamily(Enum):\n",
    "    \"\"\"Represents the model families available for selection.\"\"\"\n",
    "    \n",
    "    GPT = 'OpenAI GPT'\n",
    "    GEMINI = 'Google Gemini'\n",
    "    CLAUDE = 'Anthropic Claude'\n",
    "    MISTRAL = 'MistralAI Mistral'\n",
    "    LLAMA = 'Meta Llama'\n",
    "\n",
    "class ModelName(Enum):\n",
    "    \"\"\"\n",
    "    Enum representing different model names.\n",
    "    Each model name is associated with a model family and a model identifier.\n",
    "    \"\"\"\n",
    "      \n",
    "    LLAMA3_1_405B_INSTRUCT = (ModelFamily.LLAMA, 'llama-3.1-405b-instruct')\n",
    "    LLAMA3_1_70B_INSTRUCT = (ModelFamily.LLAMA, 'llama-3.1-70b-instruct')\n",
    "    LLAMA3_1_8B_INSTRUCT = (ModelFamily.LLAMA, 'llama-3.1-8b-instruct')\n",
    "    GPT_3_5_TURBO = (ModelFamily.GPT, 'gpt-3.5-turbo')\n",
    "    GPT_4O = (ModelFamily.GPT, 'gpt-4o')\n",
    "    GPT_4_TURBO = (ModelFamily.GPT, 'gpt-4-turbo')\n",
    "    MISTRAL_LARGE = (ModelFamily.MISTRAL, 'mistral-large')\n",
    "    OPEN_MIXTRAL_8X22B = (ModelFamily.MISTRAL, 'open-mixtral-8x22b')\n",
    "    MISTRAL_SMALL = (ModelFamily.MISTRAL, 'mistral-small')\n",
    "    GEMINI_1_5_FLASH = (ModelFamily.GEMINI, 'gemini-1.5-flash')\n",
    "    GEMINI_1_5_PRO = (ModelFamily.GEMINI, 'gemini-1.5-pro')\n",
    "    CLAUDE_3_HAIKU = (ModelFamily.CLAUDE, 'claude-3-haiku')\n",
    "    CLAUDE_3_5_SONNET = (ModelFamily.CLAUDE, 'claude-3.5-sonnet')\n",
    "    CLAUDE_3_OPUS = (ModelFamily.CLAUDE, 'claude-3-opus')\n",
    "\n",
    "\n",
    "def get_llm(model_name: ModelName, temperature: float, max_new_tokens: int) -> BaseChatModel:\n",
    "    \"\"\"\n",
    "    Returns a chat model based on the specified model name, temperature, and maximum number of new tokens.\n",
    "\n",
    "    Args:\n",
    "        model_name (ModelName): The name of the model to use.\n",
    "        temperature (float): The temperature parameter for generating responses, [0, 2].\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate in the response.\n",
    "\n",
    "    Returns:\n",
    "        BaseChatModel: The chat model based on the specified parameters.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "\n",
    "    match model_name:\n",
    "        case ModelName.LLAMA3_1_405B_INSTRUCT: \n",
    "            return ChatFireworks(\n",
    "                name=\"accounts/fireworks/models/llama-v3p1-405b-instruct\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.LLAMA3_1_70B_INSTRUCT: \n",
    "            return ChatFireworks(\n",
    "                name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.LLAMA3_1_8B_INSTRUCT:\n",
    "            return ChatFireworks(\n",
    "                name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.GPT_3_5_TURBO:\n",
    "            return ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.GPT_4O:\n",
    "            return ChatOpenAI(\n",
    "                model=\"gpt-4o\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.GPT_4_TURBO:\n",
    "            return ChatOpenAI(\n",
    "                model=\"gpt-4-turbo\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )        \n",
    "\n",
    "        case ModelName.MISTRAL_LARGE:\n",
    "            return ChatMistralAI(\n",
    "                name=\"mistral-large-latest\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.OPEN_MIXTRAL_8X22B:\n",
    "            return ChatMistralAI(\n",
    "                name=\"open-mixtral-8x22b\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.MISTRAL_SMALL:\n",
    "            return ChatMistralAI(\n",
    "                name=\"mistral-small-latest\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )        \n",
    "\n",
    "        case ModelName.GEMINI_1_5_FLASH:\n",
    "            return ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-flash\",\n",
    "                max_output_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.GEMINI_1_5_PRO:\n",
    "            return ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-pro\",\n",
    "                max_output_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.CLAUDE_3_HAIKU:\n",
    "            return ChatAnthropic(\n",
    "                model_name=\"claude-3-haiku-20240307\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )    \n",
    "\n",
    "        case ModelName.CLAUDE_3_5_SONNET:\n",
    "            return ChatAnthropic(\n",
    "                model_name=\"claude-3-5-sonnet-20240620\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case ModelName.CLAUDE_3_OPUS:\n",
    "            return ChatAnthropic(\n",
    "                model_name=\"claude-3-opus-20240229\",\n",
    "                max_tokens = max_new_tokens,\n",
    "                temperature = temperature,\n",
    "            )\n",
    "\n",
    "        case _:\n",
    "            raise RuntimeError(\"Invalid input model_name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f1297",
   "metadata": {},
   "source": [
    "## Get the model family - model name dictionary\n",
    "\n",
    "This dictionary is used to set and update the dropdown menus in the Gradio UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names by families\n",
    "model_by_families: dict[str, list[str]] = {family.value: [] for family in ModelFamily}\n",
    "for model in ModelName:\n",
    "    family, name = model.value\n",
    "    model_by_families[family.value].append(name)\n",
    "\n",
    "print(model_by_families)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0871fd",
   "metadata": {},
   "source": [
    "# Generic llm query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1144dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation prompt template\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '{system_prompt}'),\n",
    "    MessagesPlaceholder('history'),\n",
    "    ('human', '{prompt}'),\n",
    "])\n",
    "\n",
    "str_output_parser = StrOutputParser()\n",
    "\n",
    "def exec_prompt(\n",
    "        chat_history: list[list[str]] | None,\n",
    "        prompt: str,\n",
    "        system_prompt: str,\n",
    "        model_family: str ,\n",
    "        model: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int,\n",
    "        streaming: bool,\n",
    "    ) -> Iterable[tuple[list[list[str]], str]]:\n",
    "    \"\"\"\n",
    "    Executes a prompt in the chatbot system and returns the chat history and response.\n",
    "\n",
    "    Args:\n",
    "        chat_history (list[list[str]] | None): The chat history as a list of human and AI messages.\n",
    "        prompt (str): The prompt to be executed.\n",
    "        system_prompt (str): The system prompt to be used.\n",
    "        model_family (str): The model family to be used.\n",
    "        model (str): The specific model to be used.\n",
    "        temperature (float): The temperature parameter for generating responses.\n",
    "        max_tokens (int): The maximum number of tokens in the generated response.\n",
    "        streaming (bool): Whether to use streaming or not. In streaming mode, the response is generated in chunks, otherwise the response is generated in one go.\n",
    "\n",
    "    Returns:\n",
    "        Iterable[tuple[list[list[str]], str]]: An iterable of tuples containing the updated chat history and an empty string signifying that the user prompt is moved from the input field into the history.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if not prompt:\n",
    "        prompt = \"I have no question\"\n",
    "\n",
    "    if not chat_history:\n",
    "        chat_history = []\n",
    "\n",
    "    if model_family in [\"Mistral\", \"Gemini\", \"Claude\"] and temperature > 1:\n",
    "        temperature = 1\n",
    "\n",
    "    model_family_kind = ModelFamily(model_family)\n",
    "    model_name_kind = ModelName((model_family_kind, model))\n",
    "\n",
    "    llm = get_llm(\n",
    "        model_name=model_name_kind,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    for human, ai in chat_history:\n",
    "        history.append(('human', human))\n",
    "        history.append(('ai', ai))\n",
    "\n",
    "   \n",
    "    chain = chat_prompt_template | llm | str_output_parser\n",
    "\n",
    "    if streaming:\n",
    "        response_iter = chain.stream({\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"history\": history,\n",
    "            \"prompt\": prompt,\n",
    "        })\n",
    "\n",
    "        chat_history.append([prompt, \"\"])\n",
    "        for response_chunk in response_iter:\n",
    "            chat_history[-1][1] += response_chunk\n",
    "            yield (chat_history, \"\")\n",
    "\n",
    "    else:\n",
    "        response = chain.invoke({\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"history\": history,\n",
    "            \"prompt\": prompt,\n",
    "        })\n",
    "\n",
    "        chat_history.append([prompt, response])\n",
    "        yield (chat_history, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f059c8",
   "metadata": {},
   "source": [
    "## Wrappers for the gradio streaming and not streaming methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exec_prompt_wrapper_streaming(\n",
    "        chat_history: list[list[str]] | None, prompt: str, system_prompt: str, model_family: str, model: str, temperature: float, max_tokens: int\n",
    ") -> Iterable[tuple[list[list[str]], str]]:\n",
    "    \"\"\"\n",
    "    Executes a prompt in a streaming manner using the specified parameters.\n",
    "\n",
    "    This is a wrapper function around the `exec_prompt` function that sets the `streaming` parameter to `True`.\n",
    "\n",
    "    Args:\n",
    "        chat_history (list[list[str]] | None): The chat history as a list of lists of strings. Each inner list represents a message in the conversation.\n",
    "        prompt (str): The prompt to be executed.\n",
    "        system_prompt (str): The system prompt to be used.\n",
    "        model_family (str): The model family to be used.\n",
    "        model (str): The model to be used.\n",
    "        temperature (float): The temperature parameter for generating the output [0, 2].\n",
    "        max_tokens (int): The maximum number of tokens in the generated output.\n",
    "\n",
    "    Yields:\n",
    "        Iterable[tuple[list[list[str]], str]]: An iterable of tuples, where each tuple contains the updated chat history and an empty string signifying that the user prompt is moved from the input field into the history.\n",
    "    \"\"\"\n",
    "    \n",
    "    yield from exec_prompt(\n",
    "        chat_history=chat_history,\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        model_family=model_family,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "def exec_prompt_wrapper(\n",
    "    chat_history: list[list[str]] | None, prompt: str, system_prompt: str, model_family: str, model: str, temperature: float, max_tokens: int\n",
    ") -> tuple[list[list[str]], str]:\n",
    "    \"\"\"\n",
    "    Executes the prompt using the specified parameters, implicitly setting the `streaming` parameter to `False`.\n",
    "\n",
    "    This is a wrapper function around the `exec_prompt` function that sets the `streaming` parameter to `False`.\n",
    "\n",
    "    Args:\n",
    "        chat_history (list[list[str]] | None): The chat history.\n",
    "        prompt (str): The prompt to generate a response for.\n",
    "        system_prompt (str): The system prompt.\n",
    "        model_family (str): The model family.\n",
    "        model (str): The model.\n",
    "        temperature (float): The temperature for response generation [0, 2].\n",
    "        max_tokens (int): The maximum number of tokens for the response.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[list[str]], str]: A tuple containing the updated chat history and an empty string, signifying that the user prompt is moved from the input field into the history\n",
    "    \"\"\"\n",
    "\n",
    "    return next(exec_prompt(\n",
    "    chat_history=chat_history,\n",
    "    prompt=prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    model_family=model_family,\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    streaming=False,\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5daa5",
   "metadata": {},
   "source": [
    "# Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a255bdf-4f95-4ea8-987e-e72f0cf5433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n",
    "callback = gr.CSVLogger()\n",
    "\n",
    "def save_datapoint(*args):\n",
    "    callback.flag(args)\n",
    "    gr.Info(\"Data point flagged for review.\")\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    # UI Elements\n",
    "\n",
    "    gr.Markdown(\"# Component Soft LLM Demo\")\n",
    "    system_prompt = gr.Textbox(label=\"System prompt\", value=\"You are a helpful, harmless and honest assistant.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_family = gr.Dropdown(\n",
    "            choices=list(model_by_families.keys()),\n",
    "            label=\"Model family\",\n",
    "            value=\"OpenAI GPT\",\n",
    "        )\n",
    "\n",
    "        model_name = gr.Dropdown(\n",
    "            choices=list(model_by_families[model_family.value]),\n",
    "            label=\"Model\",\n",
    "            value=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "        temperature = gr.Slider(\n",
    "            label=\"Temperature:\",\n",
    "            minimum=0,\n",
    "            maximum=2,\n",
    "            value=1,\n",
    "            info=\"LLM generation temperature\",\n",
    "        )\n",
    "\n",
    "        max_tokens = gr.Slider(\n",
    "            label=\"Max tokens\",\n",
    "            minimum=100,\n",
    "            maximum=2000,\n",
    "            value=500, \n",
    "            info=\"Maximum number of generated tokens\",\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"ComponentSoft_GPT\",\n",
    "            height=400,\n",
    "            show_copy_button=True,\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(\n",
    "            label=\"Your prompt\",\n",
    "            value=\"Who was Albert Einstein?\",\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(value=\"Answer without streaming\")\n",
    "        submit_btn_streaming = gr.Button(value=\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot])\n",
    "        flag_btn = gr.Button(\"Flag\")\n",
    "\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"Who was Albert Einstein?\",\n",
    "            \"When did he live?\",\n",
    "            \"What were a few of his most important achievements?\",\n",
    "            \"Who were some other important personality from his profession and his age?\",\n",
    "            \"Write a Python function which calculates the value of PI in N steps with maximum precision using float64 numbers.\",\n",
    "            \"Write the same function in Typescript.\",\n",
    "            \"The same in Java?\",\n",
    "            \"And what about C#?\",\n",
    "            \"In Fortran?\",\n",
    "            \"In Cobol?\"\n",
    "        ],\n",
    "        inputs=prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    # Event listeners\n",
    "    model_family.change(\n",
    "            fn=lambda family: gr.Dropdown(\n",
    "                choices=list(model_by_families[family]),\n",
    "                label=\"Model\",\n",
    "                value=model_by_families[family][0],\n",
    "                interactive=True,\n",
    "            ),\n",
    "            inputs=model_family,\n",
    "            outputs=model_name,\n",
    "    )\n",
    "\n",
    "    submit_btn_streaming.click(\n",
    "        fn=exec_prompt_wrapper_streaming,\n",
    "        inputs=[chatbot, prompt, system_prompt, model_family, model_name, temperature, max_tokens],\n",
    "        outputs=[chatbot, prompt],\n",
    "    )\n",
    "\n",
    "    submit_btn_nostreaming.click(\n",
    "        fn=exec_prompt_wrapper,\n",
    "        inputs=[chatbot, prompt, system_prompt, model_family, model_name, temperature, max_tokens],\n",
    "        outputs=[chatbot, prompt],\n",
    "    )\n",
    "\n",
    "    flag_btn.click(\n",
    "        fn=save_datapoint,  # type: ignore\n",
    "        inputs=[system_prompt, model_family, model_name, temperature, max_tokens, chatbot],\n",
    "        preprocess=False,\n",
    "    )\n",
    "\n",
    "    callback.setup([system_prompt, model_family, model_name, temperature, max_tokens, chatbot], \"flagged_data_points\")\n",
    "\n",
    "#demo.launch()\n",
    "demo.launch(share=True)\n",
    "#demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(GRADIO_USER, GRADIO_PASSWORD), max_threads=20, show_error=True, favicon_path=\"favicon.ico\", state_session_capacity=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "985e1a2e-40f9-4955-a2cd-8183f133c5d5",
   "metadata": {},
   "source": [
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
